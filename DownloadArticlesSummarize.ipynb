{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape news articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.489230Z",
     "start_time": "2020-07-18T22:30:36.250214Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3 \n",
    "import os\n",
    "from io import StringIO\n",
    "from summarizer import Summarizer\n",
    "import pickle\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.505215Z",
     "start_time": "2020-07-18T22:30:40.496217Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.520215Z",
     "start_time": "2020-07-18T22:30:40.510213Z"
    }
   },
   "outputs": [],
   "source": [
    "# API Key for News API\n",
    "secret = '<key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.536221Z",
     "start_time": "2020-07-18T22:30:40.525215Z"
    }
   },
   "outputs": [],
   "source": [
    "# s3 secret\n",
    "# s3 secret\n",
    "ACCESS_KEY ='<key>'\n",
    "SECRET_KEY = '<key>'\n",
    "BUCKET_NAME ='<key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.551218Z",
     "start_time": "2020-07-18T22:30:40.541217Z"
    }
   },
   "outputs": [],
   "source": [
    "jdbcUrl='<key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.846219Z",
     "start_time": "2020-07-18T22:30:40.555215Z"
    }
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ").Bucket(BUCKET_NAME)\n",
    "\n",
    "json.load_s3 = lambda f: json.load(s3.Object(key=f).get()[\"Body\"])\n",
    "json.dump_s3 = lambda obj, f: s3.Object(key=f).put(Body=json.dumps(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.862217Z",
     "start_time": "2020-07-18T22:30:40.850219Z"
    }
   },
   "outputs": [],
   "source": [
    "def savePickle(object, filename, protocol = pickle.HIGHEST_PROTOCOL):\n",
    "    pickle.dump(object, open(filename, \"wb\",),protocol)\n",
    "\n",
    "def loadPickle(filename):\n",
    "    return pickle.load(open(filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.877215Z",
     "start_time": "2020-07-18T22:30:40.867217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the endpoint to extract all the top headlines \n",
    "url = 'https://newsapi.org/v2/top-headlines?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.893219Z",
     "start_time": "2020-07-18T22:30:40.885217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the query and number of returns - Limit the headlines to country US, for now\n",
    "parameters = {\n",
    "    'language': 'en',\n",
    "    'country':'us',\n",
    "    'pageSize': 100,\n",
    "    'apiKey': secret \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:40.909217Z",
     "start_time": "2020-07-18T22:30:40.899221Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['business','entertainment','general','health','science','sports','technology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T22:30:41.776214Z",
     "start_time": "2020-07-18T22:30:40.926217Z"
    }
   },
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "\n",
    "article_title = []\n",
    "article_authors = []\n",
    "article_text = []\n",
    "article_summary = []\n",
    "article_date = []\n",
    "article_top_image = []\n",
    "failed_url = []\n",
    "category_articles ={}\n",
    "pages= range(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.285Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.287Z"
    }
   },
   "outputs": [],
   "source": [
    "count =0\n",
    "#model = Summarizer()\n",
    "first = True\n",
    "for category in categories:\n",
    "    name = category\n",
    "    print(name)\n",
    "    fileName= f'resources/{category}_top_headline_data_new.json'\n",
    "    json_buffer=json.load_s3('Project3/'+fileName)\n",
    "    category_dataframe = pd.read_json(json_buffer).T    \n",
    "    parameters['category'] = category\n",
    "    response = requests.get(url, params=parameters)\n",
    "    if response.status_code != requests.codes.ok:\n",
    "        print(f\"Bad result : {response.url}\")\n",
    "        continue\n",
    "    response_json = response.json()\n",
    "\n",
    "    df = pd.DataFrame.from_dict(response_json)\n",
    "    df = pd.concat([df.drop(['articles'], axis=1), df['articles'].apply(pd.Series)], axis=1) \n",
    "\n",
    "    rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        h = row['url']\n",
    "        # skip record if we already downloaded article\n",
    "        if h in category_dataframe['url'].values:\n",
    "            continue \n",
    "        user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "        config = Config()\n",
    "        config.verbose=True\n",
    "        config.browser_user_agent = user_agent\n",
    "        newsarticle = Article(h, config=config)\n",
    "        try :\n",
    "            newsarticle.download()\n",
    "            newsarticle.parse()\n",
    "            if not newsarticle.text :\n",
    "                print(f\"Unable to parse article: {h}\")\n",
    "                continue \n",
    "            artdict = {}                \n",
    "            artdict['articleText']=newsarticle.text\n",
    "            # Get article Summary            \n",
    "            summaryResult = model(newsarticle.text, min_length=60)\n",
    "            full_summary = ''.join(summaryResult)\n",
    "            if not full_summary :\n",
    "                #no Summary was generated\n",
    "                artdict['articleSummary']=''\n",
    "            else:\n",
    "                row['articleSummary']=full_summary\n",
    "            artdict['articleSentiment']='' # Place Holder for calculated Sentiment Analysis\n",
    "            for column in df:\n",
    "                #print(column)\n",
    "                if column in ['status', 'totalResults','content'] :\n",
    "                    continue \n",
    "                artdict[column]= row[column]\n",
    "            rows.append(artdict)\n",
    "            count += 1\n",
    "            if (count % 50 == 0 ):\n",
    "                print('Number of articles: '+str(count))\n",
    "\n",
    "        except :\n",
    "             print(f'***FAILED TO DOWNLOAD***{newsarticle.url}')\n",
    "             pass\n",
    "\n",
    "    print(f\"Using DataFrame {category_dataframe.shape} Adding {len(rows)}\" )\n",
    "    if len(rows) > 0 :\n",
    "        category_dataframe = category_dataframe.append(pd.DataFrame(rows),ignore_index=True).drop_duplicates(subset='url')\n",
    "        \n",
    "    if first:\n",
    "        complete_dataframe=category_dataframe\n",
    "        first = False \n",
    "    else :\n",
    "        df = pd.read_json(json_buffer).T\n",
    "        complete_dataframe = complete_dataframe.append(category_dataframe,ignore_index=True)        \n",
    "\n",
    "    json_buffer = StringIO()\n",
    "    category_dataframe.to_json(json_buffer, orient='index')\n",
    "    json.dump_s3(json_buffer.getvalue(),'Project3/'+fileName)\n",
    "\n",
    "print('Total number of articles: '+str(count))\n",
    "print(complete_dataframe.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.289Z"
    }
   },
   "outputs": [],
   "source": [
    "print(complete_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.292Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.svm import SVC \n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.297Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataframe['articleSummary'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.300Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_words = ['i', 'your', 'you', 'on', 'with', 'and', 'have', 'the', 'to', 'in', 'for', 'that', 'had', 'be', 'a', 'year'\n",
    "               'it', 'may', 'one', 'as', 'if', 'is', 'via', 'this', 'will', 'david', 'jenni', 'im', 'susan', 'it', 'up',\n",
    "               'angelica', 'hi', 'hello', 'we', 'our', 'all', 'kelli', 'yes', 'gavin', 'our', 'were', 'of', 'can', 'at',\n",
    "               'any', 'by', 'also', 'joe', 'ronnie', 'morning', 'evening', 'good', 'what', 'okay', 'ok', 'are', 'us', 'my',\n",
    "               'th', 'st', 'nd', 'rd', 'was', 'there', 'then', 'lee', 'out', 'or', 'so','alan', 'from','unfortunately',\n",
    "               'alason', 'but', 'youre', 'does', 'heres', 'little', 'more', 'set', 'br', 'dr', 'ave', 'here', 'about', 'an',\n",
    "               'let', 'know', 'than', 'then', 'no', 'why', 'way', 'every', 'thats', 'taken', 'today', 'way', 'id', 'isnt', \n",
    "               'only', 'bay', 'me', 'when', 'want', 'end', 'month', 'do', 'til', 'get', 'back', 'thanks', 'bonnie', 'woodal',\n",
    "               'off', 'drive', 'michell','and', 'he','she', 'her','just']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.302Z"
    }
   },
   "outputs": [],
   "source": [
    "print(complete_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.304Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round(text):\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '',text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[''\"\"...]','',text)\n",
    "    text = re.sub('\\n','',text)\n",
    "    text = re.sub('\\d+', '', text)\n",
    "    text = re.sub('[^\\w\\s]','', text)  # remove punctuation \n",
    "    \n",
    "    return text\n",
    "\n",
    "complete_dataframe.articleSummary = complete_dataframe.articleSummary.str.replace('\\d+', '') # remove numbers\n",
    "get_cleaning = lambda x: clean_text_round(x)\n",
    "complete_dataframe.articleSummary = pd.DataFrame(complete_dataframe.articleSummary.apply(get_cleaning))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.307Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataframe['articleSummary'] = complete_dataframe['articleSummary'].apply(lambda x: gensim.parsing.preprocessing.remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.309Z"
    }
   },
   "outputs": [],
   "source": [
    "print(complete_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.312Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_remove_words_to_texts():\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))\n",
    "    complete_dataframe[\"articleSummary_new\"] = complete_dataframe['articleSummary'].str.replace(pat, '')\n",
    "    complete_dataframe[\"articleSummary_new\"] = complete_dataframe['articleSummary_new'].str.strip()\n",
    "    return complete_dataframe\n",
    "complete_dataframe = apply_remove_words_to_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.315Z"
    }
   },
   "outputs": [],
   "source": [
    "print(complete_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.317Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataframe = apply_remove_words_to_texts()\n",
    "complete_dataframe.articleSummary_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.319Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(min_df = 5,\n",
    "#                              max_df = 0.8,\n",
    "#                              sublinear_tf = True,\n",
    "#                              use_idf = True)\n",
    "\n",
    "vectorizer = loadPickle('final_model/vectorizer.sav')\n",
    "\n",
    "news_vectors = vectorizer.transform(complete_dataframe['articleSummary_new']).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.320Z"
    }
   },
   "outputs": [],
   "source": [
    "news_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.322Z"
    }
   },
   "outputs": [],
   "source": [
    "svm_model = loadPickle('final_model/gensim_svm_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.324Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted = svm_model.predict(news_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.326Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataframe['articleSentiment']=predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.329Z"
    }
   },
   "outputs": [],
   "source": [
    "# return source Name\n",
    "complete_dataframe['source'] =complete_dataframe['source'].apply(lambda x: x['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.333Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.336Z"
    }
   },
   "outputs": [],
   "source": [
    "# set date\n",
    "complete_dataframe['publishedAt']= pd.to_datetime(complete_dataframe['publishedAt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data to postgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.340Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataframe.rename(columns = {'urlToImage':'urltoimage', \n",
    "                           'publishedAt':'publishedat',\n",
    "                           'articleSummary':'articlesummary',\n",
    "                           'articleSentiment':'articlesentiment'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.342Z"
    }
   },
   "outputs": [],
   "source": [
    "alchemyEngine   = create_engine(jdbcUrl, pool_recycle=3600);\n",
    "postgreSQLConnection    = alchemyEngine.connect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.346Z"
    }
   },
   "outputs": [],
   "source": [
    "postgreSQLTable= \"sentiment_results\"  \n",
    "frame= complete_dataframe[['author', 'title', 'description', 'url',\n",
    "       'urltoimage', 'publishedat', 'articlesummary', 'articlesentiment',\n",
    "       'category','source']].to_sql(postgreSQLTable, postgreSQLConnection, if_exists='replace');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.349Z"
    }
   },
   "outputs": [],
   "source": [
    "fileName= f'resources/all_top_headline_data_new.json'\n",
    "json_buffer = StringIO()\n",
    "complete_dataframe.to_json(json_buffer, orient='index')\n",
    "json.dump_s3(json_buffer.getvalue(),'Project3/'+fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-18T22:30:36.354Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_dataframe.to_json(\"all_top_headline_data_new.json\", orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
